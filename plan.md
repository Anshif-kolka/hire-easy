ğŸ“˜ HIRING AI AGENT â€“ PROJECT ARCHITECTURE DOCUMENTATION

hiring-ai-agent/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ dependencies.py
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ jd_context_agent.py
â”‚   â”‚   â”œâ”€â”€ resume_analysis_agent.py
â”‚   â”‚   â”œâ”€â”€ ranking_agent.py
â”‚   â”‚   â”œâ”€â”€ browser_agent.py
â”‚   â”‚   â””â”€â”€ email_ingest_agent.py
â”‚   â”‚
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ job_context_workflow.py
â”‚   â”‚   â”œâ”€â”€ resume_ingestion_workflow.py
â”‚   â”‚   â”œâ”€â”€ assessment_workflow.py
â”‚   â”‚   â””â”€â”€ ranking_workflow.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ gemini_llm.py
â”‚   â”‚   â”œâ”€â”€ chroma_db.py
â”‚   â”‚   â”œâ”€â”€ pdf_parser.py
â”‚   â”‚   â”œâ”€â”€ resume_extractor.py
â”‚   â”‚   â”œâ”€â”€ safe_link_checker.py
â”‚   â”‚   â””â”€â”€ scoring_utils.py
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ job_context.py
â”‚   â”‚   â”œâ”€â”€ candidate.py
â”‚   â”‚   â””â”€â”€ score_report.py
â”‚   â”‚
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ store.py
â”‚   â”‚   â””â”€â”€ schemas.sql
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ text_cleaner.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ error_handler.py
â”‚   â”‚
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ routes_job.py
â”‚       â”œâ”€â”€ routes_resume.py
â”‚       â””â”€â”€ routes_ranking.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â”œâ”€â”€ test_db.py
â”‚   â””â”€â”€ test_api.py
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ jd_context.txt
â”‚   â”œâ”€â”€ resume_analysis.txt
â”‚   â”œâ”€â”€ ranking.txt
â”‚   â””â”€â”€ safety_browser.txt
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .env


#ï¸âƒ£ 2. DIRECTORY-WISE EXPLANATION

ğŸ“‚ agents/ â€” All Autonomous Agents

jd_context_agent.py â†’ 
Purpose:

Extract job role context from a conversation between the hiring manager and the system.

When triggered:

When a hiring manager submits job details (chat, form, text).

Main responsibilities:

Understand the job description or hiring manager messages

Extract structured data (role, experience, must-have skills, nice-to-have skills, soft skills, domain)

Clean noisy descriptions

Generate a final JobContext model
class JDContextAgent:
    def __init__(self, llm):
        self.llm = llm

    def extract_job_context(self, raw_text: str) -> JobContext:
        """
        Calls LLM to convert messy JD text into structured fields.
        """
LLM duties:

Break down messy JD text

Normalize skill names

Infer missing details (e.g., domain, seniority level)

Write a summary of the job

Tag key skills by category
Typical output:
{
  "role": "AI Engineer",
  "experience_required": 3,
  "mandatory_skills": ["Python", "FastAPI", "LLMs", "LangChain"],
  "optional_skills": ["Docker", "RAG systems"],
  "domain": "Generative AI",
  "job_summary": "Looking for an engineer to build LLM-driven workflows..."
}


resume_analysis_agent.py â†’ Parses resume, extracts structured candidate information
Purpose:

Convert a candidateâ€™s resume into structured machine-readable data.

Triggered when:

A resume (PDF/text/image) is uploaded.

pdf_parser.py extracts raw text.

Main responsibilities:

Clean raw text

Extract:

name

contact info

experience years

projects

skills

work history

education

Infer missing values with LLM reasoning

Produce a Candidate model object
class ResumeAnalysisAgent:
    def __init__(self, llm):
        self.llm = llm

    def parse_resume(self, raw_resume_text: str) -> Candidate:
        """
        Extract structured candidate data using the LLM.
        """
LLM duties:

Identify key sections even if resume layout is inconsistent

Normalize skill names

Extract project descriptions

Provide clean summaries (technical + non-technical)
output: {
  "name": "Rahul Nair",
  "email": "rahul@example.com",
  "skills": ["Python", "Docker", "LangChain"],
  "experience_years": 2.5,
  "projects": ["RAG-based FAQ bot", "Image classifier"],
  "education": "B.Tech CSE"
}


ranking_agent.py â†’ Computes match score + strengths/weaknesses
Purpose:

Evaluate each candidate based on job context + resume and generate:

ğŸ”¹ Match score
ğŸ”¹ Pros & Cons (written by LLM)
ğŸ”¹ Skill mismatch reasoning
ğŸ”¹ Final shortlisting recommendation

Triggered when:

Multiple candidates are processed

JD context is already known

Main responsibilities:

Compare candidate skills â†” job skills

Compute structured scores:

skill match

experience match

domain match

Ask LLM to:

Generate natural language pros & cons

Create a final ranking score

Give reasoning

Core methods inside:
class RankingAgent:
    def __init__(self, llm):
        self.llm = llm

    def generate_candidate_rank(self, candidate: Candidate, job: JobContext) -> ScoreReport:
        """
        Combines heuristic scoring + LLM reasoning to generate ranking output.
        """
LLM duties:

Explain strengths & weaknesses

Identify gaps

Produce a final score and explanation

Compare multiple candidates if needed

Typical output:
{
  "candidate_name": "Rahul Nair",
  "score": 82,
  "pros": ["Strong Python background", "Hands-on LLM integrations"],
  "cons": ["Limited system design experience"],
  "reasoning": "Overall strong match for an AI engineer role..."
}

browser_agent.py â†’ Visits GitHub/LinkedIn safely
Purpose:

Safely visit GitHub/LinkedIn and extract relevant publicly available data.

Triggered when:

Candidate provides GitHub/LinkedIn URLs

Your workflow needs additional signals

Main responsibilities:

Use safe_link_checker.py to avoid unsafe URLs

Scrape (lightweight):

GitHub contribution score

Number of repos

Main languages

LinkedIn bio summary

Pass cleaned text to LLM for:

Summaries

Code quality remarks

Repo expertise categorization

Core methods inside:
class BrowserAgent:
    def __init__(self, llm, browser):
        self.llm = llm
        self.browser = browser

    def scan_profile(self, url: str) -> dict:
        """
        Scrape + analyze GitHub/LinkedIn.
        """
LLM duties:

Summarize GitHub profile

Extract candidate signals (AI experience, writing quality, recency of contributions)

email_ingest_agent.py â†’ Reads resumes from email inbox

Purpose:

Automatically fetch resumes from a monitored email inbox.

Triggered when:

HR forwards resumes to a specific email ID

Cron job or manual API call triggers ingestion

Main responsibilities:

Connect to IMAP inbox

Parse attachments

Extract PDF resumes

Store metadata in DB

Pass raw PDFs â†’ pdf_parser.py â†’ LLM for analysis

Avoid duplicates

Core methods inside:
class EmailIngestAgent:
    def __init__(self, email_client, parser):
        self.email_client = email_client
        self.parser = parser

    def fetch_and_process_resumes(self):
        """
        Fetch unread emails, extract resumes, parse them.
        """
LLM duties:

None directly
LLM is used downstream in the resume analysis agent.
ğŸ“‚ workflows/ â€” Multi-step orchestrated processes

job_context_workflow.py â†’ JD ingestion â†’ extract â†’ embed â†’ store
Purpose

Convert a messy job description (JD) or hiring-manager conversation into:

A structured JobContext model

Embeddings

Stored job profile in vector DB + normal DB

Used Agents / Services

JDContextAgent

chroma_db.py (embedding + upsert)

gemini_llm.py (LLM for extraction)

Workflow Steps

Receive raw JD text or chat transcript

Call JDContextAgent.extract_job_context()

Generate embeddings of:

Mandatory skills

Optional skills

Job summary

Store:

Structured job context in DB

Embeddings in Chroma/Pinecone

Return a clean JobContext object + job_id

Outpu:
{
  "job_id": "JOB-2024-001",
  "role": "AI Engineer",
  "mandatory_skills": ["Python", "LLMs", "FastAPI"],
  "optional_skills": ["RAG", "Docker"],
  "summary": "We are hiring an engineer who can build AI workflows..."
}


resume_ingestion_workflow.py â†’ Parse â†’ embed â†’ store

Purpose

Process incoming resumes (PDF/text), convert them into structured candidate profiles, embed them, and store them.

Used Agents / Services

EmailIngestAgent (optional)

ResumeAnalysisAgent

pdf_parser.py

resume_extractor.py

chroma_db.py

Workflow Steps

Fetch resumes:

from email inbox (IMAP)

OR from API upload

Extract text from PDF â†’ pdf_parser.py

Pass text to ResumeAnalysisAgent â†’ Candidate model

Embed candidate skills, experience, summary

Store:

candidate profile in DB

embeddings in vector DB

Return Candidate model

Output:
{
  "candidate_id": "CAND-552",
  "name": "Rahul Nair",
  "skills": ["Python", "LangChain"],
  "experience": 2.5,
  "projects": ["LLM chatbot", "RAG system"],
  "vector_id": "vec_11224"
}


assessment_workflow.py â†’ Compare resume vs JD â†’ scoring
Purpose

Compare one candidate vs one job role using:

Skill match

Experience match

LLM reasoning (gap analysis)

Generate a raw score (pre-ranking)

Used Agents / Services

RankingAgent (scoring logic + LLM-based pros/cons)

scoring_utils.py

Workflow Steps

Load job context by job_id

Load candidate data by candidate_id

Compute:

skill overlap

experience gap

domain similarity

Ask RankingAgent to generate:

overall score

pros

cons

reasoning

Output:

{
  "candidate_id": "CAND-552",
  "job_id": "JOB-2024-001",
  "score": 82,
  "pros": ["Strong Python skills", "LLM project experience"],
  "cons": ["Limited cloud exposure"],
  "reasoning": "Good match for backend AI development..."
}

ranking_workflow.py â†’ Full ranking pipeline
Purpose

Generate final ranking for all candidates for a job.
This is the full end-to-end ranking system over multiple profiles.

Used Agents / Services

RankingAgent

Vector DB (semantic candidate search)

Normal DB (candidate/job lookup)

Workflow Steps

Load job context

Query vector DB for top-K relevant candidates

For each candidate:

Run assessment_workflow logic

Sort candidates by score

Generate LLM-based:

final ranking summary

best candidates

role fit explanation

Return final ranking report

Output:
{
  "job_id": "JOB-2024-001",
  "ranking": [
    {
      "candidate_id": "C1",
      "score": 91,
      "pros": [...],
      "cons": [...]
    },
    {
      "candidate_id": "C2",
      "score": 78
    }
  ],
  "final_summary": "Top candidate is best fit due to strong ML experience..."
}

ğŸ“‚ services/ â€” Shared reusable logic

gemini_llm.py â†’ Unified LLM wrapper
Purpose

Central wrapper around Gemini API (or any LLM) so the rest of the project never interacts with raw API calls directly.

What it should contain

Class: GeminiLLM

Methods:

generate_text(prompt, temperature, ...)

generate_structured(output_schema, prompt)

embed_text(text)

Automatic:

retry logic

rate limit handling

input token trimming

error normalisation

Why?

All agents use LLMs â†’ but we donâ€™t want duplicated messy API calls in every agent.
One clean interface.
chroma_db.py â†’ Vector store operations
Purpose

Wrapper around ChromaDB to store and retrieve embeddings.

What it should contain

Class: ChromaStore

Methods:

add_embeddings(collection_name, ids, embeddings, metadata)

query_similar(collection_name, embedding, top_k)

delete(collection_name, ids)

create_or_load(collection_name)

Used by

JD ingestion workflow

Resume ingestion workflow

Ranking workflow

Why?

Agents must not write DB code.
All vector DB operations remain centralized.

pdf_parser.py â†’ Extract raw text + metadata from resumes
Purpose

Takes any uploaded PDF (resume, LinkedIn profile, portfolio PDF) and extracts text.

What it should contain

Function: extract_text_from_pdf(file_path)

Optional:

page-wise extraction

image-based OCR fallback via Tesseract (if needed)

PDF metadata extraction

Additionally

Detect:

LinkedIn-style PDFs

Resume PDFs

Scanned documents

Output example:
{
  "raw_text": "...",
  "pages": ["page1 text", "page2 text"],
  "metadata": {...}
}


resume_extractor.py â†’ Clean + structure resume text
Purpose

Uses LLM + heuristics to convert raw resume text into structured JSON.

What it should contain

Class: ResumeExtractor

Methods:

clean_text(raw_text)

parse_with_llm(raw_text) â†’ calls GeminiLLM.generate_structured()

detect_sections(raw_text)

extract_skills(raw_text) (regex + LLM)

extract_experience(raw_text)

extract_education(raw_text)

safe_link_checker.py â†’ Block dangerous URLs before opening
Purpose

Before the browser agent visits GitHub or portfolio websites,
this service ensures the URL is:

safe

not phishing

not a malware domain

not performing downloads

not LinkedIn (blocked)

What it should contain

Function: is_safe(url: str) -> bool

Checks:

Allowed domain list (github.com, gitlab.com, personal domains)

Blocked domain list (LinkedIn, login pages, shortened links)

Detect suspicious patterns:

exe, zip, rar, auto-download links

javascript injection

known phishing lookups.
scoring_utils.py â†’ Match scoring, weights, similarity functions
Purpose

Compute candidate scores based on similarity, skill match, seniority match, and experience relevance.

What it should contain

Functions:

semantic_similarity(a, b) using embeddings

calculate_skill_match(resume_skills, jd_skills)

experience_alignment(resume_exp, jd_exp)

weight_based_score(similarity, skill_match, experience_match)

final_score(resume_structured, jd_structured, embeddings_store)

Optional

Reusable scoring weights:
WEIGHTS = {
    "semantic_similarity": 0.5,
    "skills": 0.3,
    "experience": 0.2
}
Output
{
  "score": 84.5,
  "skill_match": 78,
  "experience_match": 90,
  "summary": "Strong fit, lacks AWS but good overall."
}

ğŸ“‚ models/ â€” Pydantic data models

job_context.py â†’ JD structure
Purpose

Stores the cleaned, structured representation of a job role extracted by the JD Context Agent.

What it should contain

A Pydantic model like:
from pydantic import BaseModel
from typing import List, Optional

class JobContext(BaseModel):
    job_title: str
    seniority: str
    required_skills: List[str]
    preferred_skills: List[str]
    responsibilities: List[str]
    experience_required: Optional[str]
    domain: Optional[str]
    embedding_vector: Optional[List[float]]  # stored after embedding
Usage

For storing JD embeddings in Chroma

For comparing with resumes

For ranking candidates

candidate.py â†’ Candidate profile
Purpose

Stores all extracted resume information after parsing + cleaning.

It contains:
from pydantic import BaseModel
from typing import List, Optional

class Candidate(BaseModel):
    id: str
    name: Optional[str]
    email: Optional[str]
    phone: Optional[str]
    headline: Optional[str]

    skills: List[str]
    experience: List[str]   # raw or structured
    education: List[str]
    projects: List[str]

    github_url: Optional[str]
    portfolio_url: Optional[str]
    linkedin_pdf: bool = False  # if resume came from LinkedIn export

    embedding_vector: Optional[List[float]]
Used by

Resume Analysis Agent

Assessment Workflow

Scoring Utilities

Ranking Agent

score_report.py â†’ Ranking results
Purpose

Represents the final evaluation of a candidate after scoring & ranking.

It contains
from pydantic import BaseModel
from typing import List, Optional

class ScoreReport(BaseModel):
    candidate_id: str
    score: float
    skill_match: float
    experience_match: float
    semantic_similarity: float

    strengths: List[str]
    weaknesses: List[str]

    summary: str
Used by

Ranking Agent

Ranking Workflow

API Response (for frontend dashboards)
ğŸ“‚ api/ â€” FastAPI Routes

routes_job.py â†’ Create job context
Endpoints
POST /job/create

Input: Raw JD text OR hiring manager conversation transcript

Process:

Calls JD Context Workflow

Extracts structured JD

Embeds it

Stores in vector DB

Output: JobContext object

GET /job/{job_id}

Returns job context metadata (skills, responsibilities, etc.)

routes_resume.py â†’ Upload + parse resume
Endpoints
POST /resume/upload

Input: PDF file

Process:

Extract text (pdf_parser)

Clean & structure (resume_extractor)

Generate embeddings

Store in Chroma

Output: Candidate model response

POST /resume/email-ingest

Input: Email metadata (links, attachments)

Processed by: email_ingest_agent

GET /resume/{candidate_id}

Returns parsed candidate profile.

routes_ranking.py â†’ Get ranked list
GET /ranking/{job_id}

Returns a ranked list of candidates for the job:

Process:

Load JD embeddings

Load candidate embeddings

Run assessment workflow

For each candidate:

compute score

compute strengths/weaknesses

generate summary

Ranking workflow sorts and returns

Output: List of ScoreReport

GET /ranking/{job_id}/{candidate_id}

Returns the individual score report of a candidate.

#ï¸âƒ£ 3. UML SYSTEM ARCHITECTURE DIAGRAM

+---------------------+
|       API Layer     |
| (FastAPI Endpoints) |
+---------+-----------+
          |
          v
+-----------------------------+
|         Workflows           |
|  (JD, Resume, Ranking)     |
+-------+------+--------------+
        |      |
   uses |      | orchestrates
        v      v
+---------------+   +-----------------+
|    Agents     |   |    Services     |
| (LLM-based)   |   | (Parsing, DB,   |
|               |   |  Scoring, etc)  |
+------+--------+   +--------+--------+
       |                     |
       | calls               | interacts
       v                     v
+-----------------+    +---------------+
| Gemini LLM API  |    | Vector Store  |
+-----------------+    |  (Chroma)     |
                       +---------------+
#ï¸âƒ£ 4. DATA FLOW DIAGRAM

Hiring Manager â†’ JD Context Agent â†’ Job Context Workflow
                       |
                       v
           Embeddings Stored in Vector DB
                       |
       Candidate Email Received â†’ Email Agent
                       |
                       v
Resume PDF â†’ PDF Parser â†’ Resume Agent â†’ Resume Workflow
                       |
                      (Embed + Store)
                       |
                       v
              Ranking Workflow
       (JD Embeddings + Resume Embeddings)
                       |
                       v
         Final Ranked Candidate List (API)
#ï¸âƒ£ 5. END-TO-END REQUEST â†’ WORKFLOW â†’ RESPONSE FLOW
A. Job Context Creation Flow
POST /job/create
        â”‚
        â–¼
JD Context Agent
        â”‚
Parse â†’ Extract â†’ Summarize skills
        â”‚
        â–¼
Job Context Workflow
        â”‚
Store embeddings in Chroma
        â”‚
        â–¼
Response â†’ {job_id, structured_context}


B. Resume Ingestion Flow
POST /resume/upload
        â”‚
        â–¼
PDF parser â†’ Raw text
        â”‚
        â–¼
Resume Analysis Agent
        â”‚
Extract skills, experience, projects
        â”‚
        â–¼
Resume Ingestion Workflow
        â”‚
Store embeddings
        â”‚
        â–¼
Response â†’ {resume_id, parsed_profile}

C. Ranking Flow
GET /ranking?job_id=123
        â”‚
        â–¼
Ranking Workflow
        â”‚
Fetch JD context + candidates embeddings
        â”‚
Compute similarity + scoring weights
        â”‚
Use Ranking Agent for final evaluation
        â”‚
        â–¼
Return sorted list:
[
 {candidate: X, score: 92, strengths: [], weaknesses: []},
 {candidate: Y, score: 85, ...}
]

#ï¸âƒ£ 6. AGENT INTERACTION FLOW
 JD Context Agent
        |
        v
 Resume Analysis Agent
        |
        v
 Ranking Agent <---- Browser Agent checks GitHub/LinkedIn

#ï¸âƒ£ 7. EMBEDDING + RANKING ARCHITECTURE
+------------------------+
| Job Context Embedding  |
+------------------------+

+------------------------+        +-----------------------+
| Resume Embeddings      | -----> | Similarity Matching   |
+------------------------+        +-----------------------+
                                          |
                                          v
                                Weighted Scoring + LLM
                                          |
                                          v
                                 Final Ranking Output

#ï¸âƒ£ 8. EMAIL â†’ RESUME â†’ RANKING FLOW
Email â†’ Email Agent
           |
           v
Extract PDF â†’ Resume Workflow â†’ Store â†’ Rank â†’ Output

#ï¸âƒ£ 9. FRONTEND ARCHITECTURE

The frontend is a React-based Single Page Application (SPA) built with Vite and styled with Tailwind CSS. It communicates with the FastAPI backend via REST API.

## Tech Stack
- **Framework**: React 19 (Vite)
- **Styling**: Tailwind CSS v4 (Corporate Slate/Sky palette)
- **Icons**: Lucide React (No emojis)
- **Routing**: React Router DOM v7
- **HTTP Client**: Axios

## Directory Structure
frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ client.js       # Centralized API calls
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ Layout.jsx      # Sidebar + Main Content wrapper
â”‚   â”‚   â””â”€â”€ JobForm.jsx     # Job creation form
â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”œâ”€â”€ Dashboard.jsx   # Stats + Email Trigger
â”‚   â”‚   â”œâ”€â”€ Jobs.jsx        # Job listing
â”‚   â”‚   â”œâ”€â”€ Candidates.jsx  # Candidate listing + Ranking
â”‚   â”‚   â””â”€â”€ Compare.jsx     # Side-by-side candidate comparison
â”‚   â”œâ”€â”€ App.jsx             # Routing configuration
â”‚   â”œâ”€â”€ main.jsx            # Entry point
â”‚   â””â”€â”€ index.css           # Tailwind directives + Custom utilities

## Key Features
1. **Dashboard**: Overview of system stats and manual email ingestion trigger.
2. **Job Management**: Create and view job descriptions.
3. **Candidate Management**: View parsed candidates, filter by job.
4. **Ranking System**: Trigger AI ranking for candidates against a specific job.
5. **Comparison**: Select two candidates to compare their skills, experience, and education side-by-side.

## Design Philosophy
- **Minimalistic Corporate**: Clean lines, professional typography, neutral colors (Slate/Gray) with blue accents (Sky).
- **No Emojis**: Professional iconography using Lucide React.
- **Responsive**: Adapts to different screen sizes (though primarily desktop-focused for admin use).
